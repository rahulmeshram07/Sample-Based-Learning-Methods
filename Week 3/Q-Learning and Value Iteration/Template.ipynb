{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbbae57",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- In this kernel, we will be implementing an example environment.\n",
    "- We will be deploying SARSA, Q-Learning and Expected SARSA to try and find the optimal agent's policy and the optimal value functions, in order to maximize the rewards.\n",
    "\n",
    "# Importing Packages & Boilerplate Stuff\n",
    "\n",
    "1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
    "2. numpy: the fundamental package for scientific computing with Python.\n",
    "3. matplotlib: the library for plotting graphs in Python.\n",
    "4. RL-Glue: the library for reinforcement learning experiments.\n",
    "5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.\n",
    "6. itertools.product: the function that can be used easily to compute permutations.\n",
    "7. tqdm.tqdm: Provides progress bars for visualizing the status of loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from itertools import product\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145a9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CODE\n",
    "# Setting the seed for reproducible results\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb3a9e",
   "metadata": {},
   "source": [
    "# 1. Environment\n",
    "- The below code cell provides the backbone of the `ExampleEnvironment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e40d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnvironment():\n",
    "    def __init__(self, env_info={}):\n",
    "        # These are the different possible states\n",
    "        self.grid = [0, 1, 2, 3]\n",
    "        \n",
    "        # The rewards produced by the environment in response to the different ...\n",
    "        # ... actions of the agent in different states\n",
    "        self.rewards = [\n",
    "            [0, 0, 2],\n",
    "            [0, 1, 0],\n",
    "            [1, 1, 0],\n",
    "            [2, 1.5, 3]\n",
    "        ]\n",
    "\n",
    "        # The environment is governed by the following dynamics\n",
    "        # In mathematical notation, this is nothing but p(s'|s,a)\n",
    "        # But in this example, we are assuming to be independent of actions, i.e., ...\n",
    "        # p(s'|s, a) is equal for all actions in state s\n",
    "        self.tran_matrix = np.array([\n",
    "            [1/2, 1/2, 0, 0],    # State 0\n",
    "            [1/4, 1/4, 1/2, 0],  # State 1\n",
    "            [0, 1/4, 1/4, 1/2],  # State 2\n",
    "            [0, 0, 1/4, 3/4]     # State 3\n",
    "        ])\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(env_info.get(\"seed\", 0))\n",
    "        \n",
    "        # Defines the current location\n",
    "        self.cur_loc = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.cur_loc = self.rand_generator.choice(self.grid)\n",
    "        return self.cur_loc\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_reward = self.rewards[self.cur_loc][action]\n",
    "        next_state = self.rand_generator.choice(self.grid, \n",
    "            p = self.tran_matrix[self.cur_loc])\n",
    "        self.cur_loc = next_state\n",
    "        return next_state, next_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b406c9c",
   "metadata": {},
   "source": [
    "# 2. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1822e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(theta = 0.01, discount = 0.9):\n",
    "    # Creating an instance for the environment\n",
    "    env = ExampleEnvironment()\n",
    "\n",
    "    # Defining the paramters for the simulation\n",
    "    delta = theta * 10\n",
    "\n",
    "    # Initializing the state values and the different possible actions\n",
    "    s_vals = np.zeros(4)\n",
    "    actions = list(np.arange(3))\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in env.grid:\n",
    "            cur_val = copy.copy(s_vals[s])\n",
    "            vals = []\n",
    "            for a in actions:\n",
    "                sum_rhs = env.tran_matrix[s] * (env.rewards[s][a] + discount * s_vals)\n",
    "                vals.append(np.sum(sum_rhs))\n",
    "            s_vals[s] = np.max(vals)\n",
    "            delta = max(delta, abs(cur_val - s_vals[s]))\n",
    "            \n",
    "    return s_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2033e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Convergence of Value Iteration Algorithm\n",
      "State Values:  [18.56258457 18.26038651 19.95724357 23.03054271]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "State Values:  [ 8.72378635  8.10189268  9.35294526 12.16573178]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "State Values:  [5.70583045 4.89487919 5.8361412  8.46006191]\n"
     ]
    }
   ],
   "source": [
    "s_vals = value_iteration(theta = 0.01, discount = 0.9)\n",
    "print(\"Post Convergence of Value Iteration Algorithm\")\n",
    "print(\"State Values: \", s_vals)\n",
    "\n",
    "s_vals = value_iteration(theta = 0.01, discount = 0.8)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"State Values: \", s_vals)\n",
    "\n",
    "s_vals = value_iteration(theta = 0.01, discount = 0.7)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"State Values: \", s_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e33c7",
   "metadata": {},
   "source": [
    "# 3. Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56c62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, agent_info={}):\n",
    "        # Defining the #actions and #states \n",
    "        self.num_actions = 3\n",
    "        self.num_states = 4\n",
    "        \n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\", 0.9)\n",
    "\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "\n",
    "        # To control the exploration-exploitation trade-off\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        \n",
    "        # To determine if the Q-function is converged or not\n",
    "        self.delta = agent_info.get(\"delta\", 0.01)\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\", 0))\n",
    "        \n",
    "        # Defining the initial action values\n",
    "        # self.q = self.rand_generator.randn(self.num_states, self.num_actions)\n",
    "        self.q = self.rand_generator.uniform(0, 0.1, (self.num_states, self.num_actions))\n",
    "        \n",
    "        # Initializing the variables for the previous state and action\n",
    "        self.prev_state  = None\n",
    "        self.prev_action = None\n",
    "        \n",
    "    def start(self, state):\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "            \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def step(self, state, reward):\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        # Determining the new Q-Value\n",
    "        new_val = -1e8\n",
    "        cur_val = copy.copy(self.q[self.prev_state, self.prev_action])\n",
    "        for act in range(self.num_actions):\n",
    "            val = cur_val + self.step_size * (\n",
    "                reward + self.discount * self.q[state, act] - cur_val\n",
    "            )\n",
    "            new_val = max(new_val, val)\n",
    "        self.q[self.prev_state, self.prev_action] = new_val\n",
    "            \n",
    "        # Determining if the Q-function has converged or not\n",
    "        if abs(new_val - cur_val) < self.delta:\n",
    "            return (action, True)\n",
    "        else:\n",
    "            return (action, False)\n",
    "            \n",
    "            \n",
    "    def argmax(self, q_values):\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09924af7",
   "metadata": {},
   "source": [
    "# 4. Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d64c00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env_info = {}, agent_info = {}, print_vals = True):\n",
    "    env = ExampleEnvironment(env_info) \n",
    "    agent = QLearningAgent(agent_info)\n",
    "    has_converged = False\n",
    "    num_steps = 0\n",
    "    \n",
    "    init_state  = env.start()                 # STARTING STATE\n",
    "    init_action = agent.start(init_state)     # STARTING ACTION\n",
    "    next_state, next_reward = env.step(init_action)       # STARTING REWARD\n",
    "    \n",
    "    while not has_converged:\n",
    "        next_action, has_converged = agent.step(next_state, next_reward)\n",
    "        next_state, next_reward = env.step(next_action)\n",
    "        \n",
    "        if print_vals and num_steps % 2500 == 0:\n",
    "            print(f\"Time Steps Elapsed | {num_steps}\")\n",
    "            print(\"Q-Values:\", agent.q)\n",
    "            print()\n",
    "        \n",
    "        num_steps += 1\n",
    "        \n",
    "    print(\"POST CONVERGENCE\\n\")\n",
    "    print(\"Optimal Action Values:\")\n",
    "    print(agent.q)\n",
    "    \n",
    "    print(\"\\nOptimal State Values:\")\n",
    "    print(np.max(agent.q, axis = -1))\n",
    "    \n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    print(np.argmax(agent.q, axis = -1))\n",
    "    \n",
    "    return agent.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae04f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Steps Elapsed | 0\n",
      "Q-Values: [[0.05488135 0.0648247  0.06027634]\n",
      " [0.05448832 0.04236548 0.06458941]\n",
      " [0.04375872 0.0891773  0.09636628]\n",
      " [0.03834415 0.0791725  0.05288949]]\n",
      "\n",
      "POST CONVERGENCE\n",
      "\n",
      "Optimal Action Values:\n",
      "[[0.05488135 1.57118346 0.06027634]\n",
      " [0.05448832 0.04236548 0.06458941]\n",
      " [0.04375872 0.0891773  0.09636628]\n",
      " [0.03834415 0.0791725  0.05288949]]\n",
      "\n",
      "Optimal State Values:\n",
      "[1.57118346 0.06458941 0.09636628 0.0791725 ]\n",
      "\n",
      "Optimal Policy:\n",
      "[1 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Defining the characteristics for the environment\n",
    "env_info = {\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 0.9,       \n",
    "    \"step_size\": 0.5,\n",
    "    \"epsilon\": 0.5,\n",
    "    \"delta\": 1e-3,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "q_vals = run_experiment(env_info, agent_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc69e7e",
   "metadata": {},
   "source": [
    "## 4.1. Trying to understand the effect of step-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d44066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step_sizes = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# # Defining the characteristics for the environment\n",
    "# env_info = {\n",
    "#     \"seed\": 0\n",
    "# }\n",
    "\n",
    "# # Defining the characteristics for the agent\n",
    "# agent_info = {\n",
    "#     \"discount\": 0.9,       \n",
    "#     \"step_size\": 0.1,\n",
    "#     \"epsilon\": 0.1,\n",
    "#     \"delta\": 1e-2,\n",
    "#     \"seed\": 0\n",
    "# }\n",
    "\n",
    "# state_vals_ss = []\n",
    "\n",
    "# for ss in step_sizes:\n",
    "#     print(\"\\n\\nFor step-size:\", ss)\n",
    "#     agent_info['step_size'] = ss\n",
    "#     q_vals = run_experiment(env_info, agent_info, print_vals = False)\n",
    "#     state_vals = np.max(q_vals, axis = -1)\n",
    "#     state_vals_ss.append(state_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a535654c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (10, 6))\n",
    "\n",
    "# for i in range(len(step_sizes)):\n",
    "#     plt.plot(state_vals_ss[i], label = f'Step-Size = {step_sizes[i]}')\n",
    "    \n",
    "# plt.xticks(np.arange(4))\n",
    "# plt.title(\"Optimal State Values for different step-size\")\n",
    "# plt.xlabel(\"States\")\n",
    "# plt.ylabel(\"Optimal State Values\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b080558",
   "metadata": {},
   "source": [
    "## 4.2. Trying to understand the effect of epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67e83cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Trying to understand the effect of step-size\n",
    "# eps = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "# # Defining the characteristics for the environment\n",
    "# env_info = {\n",
    "#     \"seed\": 0\n",
    "# }\n",
    "\n",
    "# # Defining the characteristics for the agent\n",
    "# agent_info = {\n",
    "#     \"discount\": 0.9,       \n",
    "#     \"step_size\": 0.1,\n",
    "#     \"epsilon\": 0.1,\n",
    "#     \"delta\": 1e-2,\n",
    "#     \"seed\": 0\n",
    "# }\n",
    "\n",
    "# state_vals_eps = []\n",
    "\n",
    "# for e in eps:\n",
    "#     print(\"\\n\\nFor epsilon:\", e)\n",
    "#     agent_info['epsilon'] = e\n",
    "#     q_vals = run_experiment(env_info, agent_info, print_vals = False)\n",
    "#     state_vals = np.max(q_vals, axis = -1)\n",
    "#     state_vals_eps.append(state_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da844a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (10, 6))\n",
    "\n",
    "# for i in range(len(eps)):\n",
    "#     plt.plot(state_vals_eps[i], label = f'Epsilon = {eps[i]}')\n",
    "    \n",
    "# plt.xticks(np.arange(4))\n",
    "# plt.title(\"Optimal State Values for different epsilon\")\n",
    "# plt.xlabel(\"States\")\n",
    "# plt.ylabel(\"Optimal State Values\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
