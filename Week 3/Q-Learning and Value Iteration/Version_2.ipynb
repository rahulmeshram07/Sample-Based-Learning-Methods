{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbbae57",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- In this kernel, we will be implementing an example environment.\n",
    "- We will be deploying SARSA, Q-Learning and Expected SARSA to try and find the optimal agent's policy and the optimal value functions, in order to maximize the rewards.\n",
    "\n",
    "# Importing Packages & Boilerplate Stuff\n",
    "\n",
    "1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
    "2. numpy: the fundamental package for scientific computing with Python.\n",
    "3. matplotlib: the library for plotting graphs in Python.\n",
    "4. RL-Glue: the library for reinforcement learning experiments.\n",
    "5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.\n",
    "6. itertools.product: the function that can be used easily to compute permutations.\n",
    "7. tqdm.tqdm: Provides progress bars for visualizing the status of loops.\n",
    "\n",
    "# Based on Version_1 (Changes)\n",
    "- Some small changes in the value iteration section. Printing some extra values, nothing special.\n",
    "- Removed the hyper-tuning cells from the end of the notebook.\n",
    "- The key change in this version is that the step-size decreases over the iterations, in Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from itertools import product\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145a9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CODE\n",
    "# Setting the seed for reproducible results\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb3a9e",
   "metadata": {},
   "source": [
    "# 1. Environment\n",
    "- The below code cell provides the backbone of the `ExampleEnvironment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e40d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnvironment():\n",
    "    def __init__(self, env_info={}):\n",
    "        # These are the different possible states\n",
    "        self.grid = [0, 1, 2, 3]\n",
    "        \n",
    "        # The rewards produced by the environment in response to the different ...\n",
    "        # ... actions of the agent in different states\n",
    "        self.rewards = [\n",
    "            [0, 0, 2],\n",
    "            [0, 1, 0],\n",
    "            [1, 1, 0],\n",
    "            [2, 1.5, 3]\n",
    "        ]\n",
    "\n",
    "        # The environment is governed by the following dynamics\n",
    "        # In mathematical notation, this is nothing but p(s'|s,a)\n",
    "        # But in this example, we are assuming to be independent of actions, i.e., ...\n",
    "        # p(s'|s, a) is equal for all actions in state s\n",
    "        self.tran_matrix = np.array([\n",
    "            [1/2, 1/2, 0, 0],    # State 0\n",
    "            [1/4, 1/4, 1/2, 0],  # State 1\n",
    "            [0, 1/4, 1/4, 1/2],  # State 2\n",
    "            [0, 0, 1/4, 3/4]     # State 3\n",
    "        ])\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(env_info.get(\"seed\", 0))\n",
    "        \n",
    "        # Defines the current location\n",
    "        self.cur_loc = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.cur_loc = self.rand_generator.choice(self.grid)\n",
    "        return self.cur_loc\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_reward = self.rewards[self.cur_loc][action]\n",
    "        next_state = self.rand_generator.choice(self.grid, \n",
    "            p = self.tran_matrix[self.cur_loc])\n",
    "        self.cur_loc = next_state\n",
    "        return next_state, next_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b406c9c",
   "metadata": {},
   "source": [
    "# 2. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1822e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(theta = 0.01, discount = 0.9):\n",
    "    # Creating an instance for the environment\n",
    "    env = ExampleEnvironment()\n",
    "\n",
    "    # Defining the paramters for the simulation\n",
    "    delta = theta * 10\n",
    "\n",
    "    # Initializing the state values and the different possible actions\n",
    "    s_vals = np.zeros(4)\n",
    "    actions = list(np.arange(3))\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in env.grid:\n",
    "            cur_val = copy.copy(s_vals[s])\n",
    "            vals = []\n",
    "            for a in actions:\n",
    "                sum_rhs = env.tran_matrix[s] * (env.rewards[s][a] + discount * s_vals)\n",
    "                vals.append(np.sum(sum_rhs))\n",
    "            s_vals[s] = np.max(vals)\n",
    "            delta = max(delta, abs(cur_val - s_vals[s]))\n",
    "            \n",
    "    return s_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49055fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Convergence of Value Iteration Algorithm\n",
      "State Values:  [18.61753617 18.31203114 20.0076184  23.08053263]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "State Values:  [ 8.74730387  8.12262406  9.37273269 12.18526228]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "State Values:  [5.71678855 4.90395976 5.84467657 8.46846691]\n"
     ]
    }
   ],
   "source": [
    "s_vals = value_iteration(theta = 0.001, discount = 0.9)\n",
    "print(\"Post Convergence of Value Iteration Algorithm\")\n",
    "print(\"State Values: \", s_vals)\n",
    "\n",
    "s_vals = value_iteration(theta = 0.001, discount = 0.8)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"State Values: \", s_vals)\n",
    "\n",
    "s_vals = value_iteration(theta = 0.001, discount = 0.7)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"State Values: \", s_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666159c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_using_q(theta = 0.01, discount = 0.9):\n",
    "    # Creating an instance for the environment\n",
    "    env = ExampleEnvironment()\n",
    "\n",
    "    # Defining the paramters for the simulation\n",
    "    delta = theta * 10\n",
    "\n",
    "    # Initializing the action values and the different possible actions\n",
    "    rand_generator = np.random.RandomState(0)\n",
    "    q_vals = rand_generator.uniform(0, 0.1, (4, 3))\n",
    "    # q_vals = np.zeros((4, 3))\n",
    "    actions = list(np.arange(3))\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in env.grid:\n",
    "            for a in actions:\n",
    "                cur_val = copy.copy(q_vals[s][a])\n",
    "                sum_rhs = 0\n",
    "                for next_s in env.grid:\n",
    "                    sum_rhs += env.tran_matrix[s][next_s] * (\n",
    "                        env.rewards[s][a] + discount * max(q_vals[next_s]) \n",
    "                    )\n",
    "                q_vals[s][a] = sum_rhs\n",
    "                delta = max(delta, abs(cur_val - q_vals[s][a]))\n",
    "\n",
    "    return q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e2033e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Convergence of Value Iteration Algorithm\n",
      "Action Values:  [[16.61795239 16.61795239 18.61795239]\n",
      " [17.31252016 18.31252016 17.31270256]\n",
      " [20.0080671  20.00820908 19.00824102]\n",
      " [22.08096246 21.58096246 23.08096246]]\n",
      "State Values:  [18.61795239 18.31252016 20.00820908 23.08096246]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "Action Values:  [[ 6.74775008  6.74775008  8.74775008]\n",
      " [ 7.12308557  8.12308557  7.12321984]\n",
      " [ 9.37314418  9.37324666  8.37326715]\n",
      " [11.1856504  10.6856504  12.1856504 ]]\n",
      "State Values:  [ 8.74775008  8.12308557  9.37324666 12.1856504 ]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "Action Values:  [[3.71658373 3.71658373 5.71658373]\n",
      " [3.90385135 4.90385135 3.90397635]\n",
      " [5.84454004 5.84463445 4.84465097]\n",
      " [7.46830996 6.96830996 8.46830996]]\n",
      "State Values:  [5.71658373 4.90385135 5.84463445 8.46830996]\n"
     ]
    }
   ],
   "source": [
    "q_vals = value_iteration_using_q(theta = 0.001, discount = 0.9)\n",
    "print(\"Post Convergence of Value Iteration Algorithm\")\n",
    "print(\"Action Values: \", q_vals)\n",
    "print(\"State Values: \", np.max(q_vals, axis = 1))\n",
    "\n",
    "q_vals = value_iteration_using_q(theta = 0.001, discount = 0.8)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"Action Values: \", q_vals)\n",
    "print(\"State Values: \", np.max(q_vals, axis = 1))\n",
    "\n",
    "q_vals = value_iteration_using_q(theta = 0.001, discount = 0.7)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"Action Values: \", q_vals)\n",
    "print(\"State Values: \", np.max(q_vals, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e4a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_q_vals = value_iteration_using_q(theta = 0.01, discount = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e33c7",
   "metadata": {},
   "source": [
    "# 3. Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c56c62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, agent_info={}):\n",
    "        # Defining the #actions and #states \n",
    "        self.num_actions = 3\n",
    "        self.num_states = 4\n",
    "        \n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\", 0.9)\n",
    "\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "\n",
    "        # To control the exploration-exploitation trade-off\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        \n",
    "        # To determine if the Q-function is converged or not\n",
    "        self.delta = agent_info.get(\"delta\", 0.01)\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\", 0))\n",
    "        \n",
    "        # Definining the Optimal Q-Values to which the algorithm should converge to\n",
    "        self.optimal_q = agent_info.get(\"optimal_q\", None)\n",
    "        \n",
    "        # Defining the initial action values\n",
    "        # self.q = self.rand_generator.randn(self.num_states, self.num_actions)\n",
    "        self.q = self.rand_generator.uniform(0, 0.1, (self.num_states, self.num_actions))\n",
    "        \n",
    "        # Initializing the variables for the previous state and action\n",
    "        self.prev_state  = None\n",
    "        self.prev_action = None\n",
    "        \n",
    "    def start(self, state, decrease_step_size = False):\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "            \n",
    "        # Reducing the step-size\n",
    "        if decrease_step_size:\n",
    "            self.step_size *= 0.8\n",
    "            \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def step(self, state, reward, decrease_step_size = False):\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "            \n",
    "        # Reducing the step-size\n",
    "        if decrease_step_size:\n",
    "            self.step_size *= 0.8\n",
    "        \n",
    "        # Determining the new Q-Value\n",
    "        new_val = -1e8\n",
    "        cur_val = copy.copy(self.q[self.prev_state, self.prev_action])\n",
    "        for act in range(self.num_actions):\n",
    "            val = cur_val + self.step_size * (\n",
    "                reward + self.discount * self.q[state, act] - cur_val\n",
    "            )\n",
    "            new_val = max(new_val, val)\n",
    "        self.q[self.prev_state, self.prev_action] = new_val\n",
    "            \n",
    "        # Determining if the Q-function has converged or not\n",
    "        if np.max(np.abs(self.optimal_q - self.q)) < self.delta:\n",
    "            return (action, True)\n",
    "        else:\n",
    "            return (action, False)\n",
    "            \n",
    "    def argmax(self, q_values):\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09924af7",
   "metadata": {},
   "source": [
    "# 4. Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d64c00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "        env_info = {}, agent_info = {}, max_iter = 1000, \n",
    "        re_init = 100, dec_step = 100, print_vals = True\n",
    "    ):\n",
    "    env = ExampleEnvironment(env_info) \n",
    "    agent = QLearningAgent(agent_info)\n",
    "    has_converged = False\n",
    "    num_iter = 0\n",
    "    \n",
    "    init_state  = env.start()                             # STARTING STATE\n",
    "    init_action = agent.start(init_state)                 # STARTING ACTION\n",
    "    next_state, next_reward = env.step(init_action)       # STARTING REWARD\n",
    "    num_iter = 1\n",
    "    \n",
    "    while not has_converged and num_iter < max_iter:\n",
    "        # After every `dec_step` steps, decrease the step-size\n",
    "        if num_iter % dec_step == 0:\n",
    "            decrease_step_size = True\n",
    "        else:\n",
    "            decrease_step_size = False\n",
    "        \n",
    "        # After every `re_init` steps, re-initialize with a random state\n",
    "        if num_iter % re_init == 0:\n",
    "            init_state  = env.start()                             \n",
    "            init_action = agent.start(init_state, decrease_step_size)                 \n",
    "            next_state, next_reward = env.step(init_action)\n",
    "        else:\n",
    "            next_action, has_converged = agent.step(next_state, next_reward, decrease_step_size)\n",
    "            next_state, next_reward = env.step(next_action)\n",
    "        \n",
    "        if print_vals and num_iter % (max_iter / 5) == 0:\n",
    "            print(f\"Time Steps Elapsed | {num_iter}\")\n",
    "            print(\"Q-Values:\", agent.q)\n",
    "            print()\n",
    "        \n",
    "        num_iter += 1\n",
    "        \n",
    "    print(\"POST CONVERGENCE\\n\")\n",
    "    print(\"Optimal Action Values:\")\n",
    "    print(agent.q)\n",
    "    \n",
    "    print(\"\\nOptimal State Values:\")\n",
    "    print(np.max(agent.q, axis = -1))\n",
    "    \n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    print(np.argmax(agent.q, axis = -1))\n",
    "    \n",
    "    return agent.q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b34cf",
   "metadata": {},
   "source": [
    "## 4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae04f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the characteristics for the environment\n",
    "env_info = {\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 0.9,       \n",
    "    \"step_size\": 0.5,\n",
    "    \"epsilon\": 0.2,\n",
    "    \"delta\": 1e-2,\n",
    "    \"optimal_q\": optimal_q_vals,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "max_iter = 200000\n",
    "re_init = 1000\n",
    "dec_step = 10000\n",
    "\n",
    "# q_vals = run_experiment(\n",
    "#     env_info, agent_info, max_iter = max_iter, re_init = re_init,\n",
    "#     dec_step = dec_step\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623b7bd",
   "metadata": {},
   "source": [
    "Time Steps Elapsed | 40000 \\\n",
    "Q-Values: [[ 0.05488135 10.71070626  0.06027634]\n",
    " [11.4383117   0.04236548 12.61760995]\n",
    " [ 0.04375872 16.03734232  7.96197394]\n",
    " [ 0.03834415 10.4194207  14.9468654 ]]\n",
    "\n",
    "Time Steps Elapsed | 80000 \\\n",
    "Q-Values: [[14.75741998 13.64244354 16.52596118]\n",
    " [13.30152604 16.23325656 16.39342828]\n",
    " [ 0.04375872 13.23629037 17.91907658]\n",
    " [ 0.03834415 10.4194207  18.12209091]]\n",
    "\n",
    "Time Steps Elapsed | 120000 \\\n",
    "Q-Values: [[15.93430712 13.64244354 16.3076523 ]\n",
    " [13.30152604 16.17647881 15.78585586]\n",
    " [ 0.04375872 15.71974405 16.12896159]\n",
    " [ 0.03834415 15.6238155  15.47263081]]\n",
    "\n",
    "Time Steps Elapsed | 160000 \\\n",
    "Q-Values: [[17.74973483 13.64244354 15.441694  ]\n",
    " [13.30152604 17.49562155 14.75383977]\n",
    " [ 0.04375872 17.50109765 16.46833595]\n",
    " [15.29431932 15.33640462 17.19304134]]\n",
    "\n",
    "POST CONVERGENCE\n",
    "\n",
    "Optimal Action Values: \\\n",
    "[[16.86760649 16.91116993 15.441694  ]\n",
    " [13.30152604 16.71770773 16.72325651]\n",
    " [16.95859037 16.50102899 16.46833595]\n",
    " [15.29431932 17.52810718 17.50259365]]\n",
    "\n",
    "Optimal State Values: \\\n",
    "[16.91116993 16.72325651 16.95859037 17.52810718]\n",
    "\n",
    "Optimal Policy: \\\n",
    "[1 2 0 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c90a74",
   "metadata": {},
   "source": [
    "## 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab53cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the characteristics for the environment\n",
    "env_info = {\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 0.9,       \n",
    "    \"step_size\": 0.5,\n",
    "    \"epsilon\": 0.2,\n",
    "    \"delta\": 1e-2,\n",
    "    \"optimal_q\": optimal_q_vals,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "max_iter = 200000\n",
    "re_init = 1000\n",
    "dec_step = 5000\n",
    "\n",
    "# q_vals = run_experiment(\n",
    "#     env_info, agent_info, max_iter = max_iter, re_init = re_init,\n",
    "#     dec_step = dec_step\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163b925",
   "metadata": {},
   "source": [
    "Time Steps Elapsed | 40000 \\\n",
    "Q-Values: [[ 0.05488135 12.53721612  0.06027634]\n",
    " [11.42090221  0.04236548 12.2878522 ]\n",
    " [ 0.04375872 14.79477624  8.68752418]\n",
    " [ 0.03834415 10.5803314  14.29430343]]\n",
    "\n",
    "Time Steps Elapsed | 80000 \\\n",
    "Q-Values: [[15.17714497 13.92537242 16.94036383]\n",
    " [14.2135258  16.2963234  16.23866067]\n",
    " [ 0.04375872 14.23049254 17.47847253]\n",
    " [ 0.03834415 10.5803314  17.63244245]]\n",
    "\n",
    "Time Steps Elapsed | 120000 \\\n",
    "Q-Values: [[17.11828702 13.92537242 16.49655773]\n",
    " [14.2135258  16.60029523 16.51277718]\n",
    " [ 0.04375872 16.53074493 16.72385805]\n",
    " [ 0.03834415 16.48081953 16.47461074]]\n",
    "\n",
    "Time Steps Elapsed | 160000 \\\n",
    "Q-Values: [[17.62497176 13.92537242 16.49655773]\n",
    " [14.2135258  17.69672259 16.51277718]\n",
    " [ 0.04375872 17.6962355  16.92261715]\n",
    " [15.20888001 17.09001532 17.54421258]]\n",
    "\n",
    "POST CONVERGENCE\n",
    "\n",
    "Optimal Action Values: \\\n",
    "[[17.73784069 14.31553089 16.49655773]\n",
    " [14.2135258  17.76716223 16.61942389]\n",
    " [ 2.68376526 17.81182415 16.92261715]\n",
    " [15.20888001 17.35728038 17.71094537]]\n",
    "\n",
    "Optimal State Values: \\\n",
    "[17.73784069 17.76716223 17.81182415 17.71094537]\n",
    "\n",
    "Optimal Policy: \\\n",
    "[0 1 1 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce08ad1",
   "metadata": {},
   "source": [
    "## 4.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e21a0e40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the characteristics for the environment\n",
    "env_info = {\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 0.9,       \n",
    "    \"step_size\": 0.5,\n",
    "    \"epsilon\": 0.2,\n",
    "    \"delta\": 1e-2,\n",
    "    \"optimal_q\": optimal_q_vals,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "max_iter = 2000000\n",
    "re_init = max_iter + 1\n",
    "dec_step = 5000\n",
    "\n",
    "# q_vals = run_experiment(\n",
    "#     env_info, agent_info, max_iter = max_iter, re_init = re_init,\n",
    "#     dec_step = dec_step\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99b1bd",
   "metadata": {},
   "source": [
    "Time Steps Elapsed | 400000 \\\n",
    "Q-Values: [[0.05488135 1.05832501 0.06027634]\n",
    " [0.05448832 0.04236548 0.06458941]\n",
    " [0.04375872 0.0891773  0.09636628]\n",
    " [0.03834415 0.0791725  0.05288949]]\n",
    "\n",
    "Time Steps Elapsed | 800000 \\\n",
    "Q-Values: [[0.05488135 1.05832418 0.06027634]\n",
    " [0.05448832 0.04236548 0.06458941]\n",
    " [0.04375872 0.0891773  0.09636628]\n",
    " [0.03834415 0.0791725  0.05288949]]\n",
    "\n",
    "Time Steps Elapsed | 1200000 \\\n",
    "Q-Values: [[0.05488135 1.05832418 0.06027634]\n",
    " [0.05448832 0.04236548 0.06458941]\n",
    " [0.04375872 0.0891773  0.09636628]\n",
    " [0.03834415 0.0791725  0.05288949]]\n",
    "\n",
    "Time Steps Elapsed | 1600000 \\\n",
    "Q-Values: [[0.05488135 1.05832418 0.06027634]\n",
    " [0.05448832 0.04236548 0.06458941]\n",
    " [0.04375872 0.0891773  0.09636628]\n",
    " [0.03834415 0.0791725  0.05288949]]\n",
    "\n",
    "POST CONVERGENCE\n",
    "\n",
    "Optimal Action Values: \\\n",
    "[[0.05488135 1.05832418 0.06027634]\n",
    " [0.05448832 0.04236548 0.06458941]\n",
    " [0.04375872 0.0891773  0.09636628]\n",
    " [0.03834415 0.0791725  0.05288949]]\n",
    "\n",
    "Optimal State Values: \\\n",
    "[1.05832418 0.06458941 0.09636628 0.0791725 ]\n",
    "\n",
    "Optimal Policy: \\\n",
    "[1 2 2 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1980e2",
   "metadata": {},
   "source": [
    "## 4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2513788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the characteristics for the environment\n",
    "env_info = {\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 0.9,       \n",
    "    \"step_size\": 0.5,\n",
    "    \"epsilon\": 0.2,\n",
    "    \"delta\": 1e-2,\n",
    "    \"optimal_q\": optimal_q_vals,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "max_iter = 200000\n",
    "re_init = max_iter / 10\n",
    "dec_step = max_iter / 20\n",
    "\n",
    "# q_vals = run_experiment(\n",
    "#     env_info, agent_info, max_iter = max_iter, re_init = re_init,\n",
    "#     dec_step = dec_step\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7b144a",
   "metadata": {},
   "source": [
    "Time Steps Elapsed | 40000 \\\n",
    "Q-Values: [[0.05488135 1.57188752 0.06027634]\n",
    " [0.05448832 0.04236548 0.06458941]\n",
    " [0.04375872 0.0891773  0.09636628]\n",
    " [0.03834415 0.0791725  0.05288949]]\n",
    "\n",
    "Time Steps Elapsed | 80000 \\\n",
    "Q-Values: [[0.05488135 2.44240942 0.06027634]\n",
    " [0.05448832 0.04236548 0.06458941]\n",
    " [0.04375872 0.0891773  0.09636628]\n",
    " [0.03834415 4.10542425 0.05288949]]\n",
    "\n",
    "Time Steps Elapsed | 120000 \\\n",
    "Q-Values: [[0.05488135 4.25162326 0.06027634]\n",
    " [0.05448832 0.04236548 0.06458941]\n",
    " [0.04375872 0.0891773  3.18243108]\n",
    " [0.03834415 4.10542425 0.05288949]]\n",
    "\n",
    "Time Steps Elapsed | 160000 \\\n",
    "Q-Values: [[0.05488135 4.25162326 0.06027634]\n",
    " [0.05448832 0.04236548 4.42800232]\n",
    " [0.04375872 0.0891773  3.18243108]\n",
    " [0.03834415 4.10542425 0.05288949]]\n",
    "\n",
    "POST CONVERGENCE\n",
    "\n",
    "Optimal Action Values: \\\n",
    "[[0.05488135 4.25162326 0.06027634]\n",
    " [0.05448832 0.04236548 4.42800232]\n",
    " [0.04375872 5.09049364 3.18243108]\n",
    " [0.03834415 4.10542425 0.05288949]]\n",
    "\n",
    "Optimal State Values: \\\n",
    "[4.25162326 4.42800232 5.09049364 4.10542425]\n",
    "\n",
    "Optimal Policy:\n",
    "[1 2 1 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
