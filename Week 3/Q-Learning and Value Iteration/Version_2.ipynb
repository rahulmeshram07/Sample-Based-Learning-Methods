{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbbae57",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- In this kernel, we will be implementing an example environment.\n",
    "- We will be deploying SARSA, Q-Learning and Expected SARSA to try and find the optimal agent's policy and the optimal value functions, in order to maximize the rewards.\n",
    "\n",
    "# Importing Packages & Boilerplate Stuff\n",
    "\n",
    "1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
    "2. numpy: the fundamental package for scientific computing with Python.\n",
    "3. matplotlib: the library for plotting graphs in Python.\n",
    "4. RL-Glue: the library for reinforcement learning experiments.\n",
    "5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.\n",
    "6. itertools.product: the function that can be used easily to compute permutations.\n",
    "7. tqdm.tqdm: Provides progress bars for visualizing the status of loops.\n",
    "\n",
    "# Based on Version_1 (Changes)\n",
    "- Some small changes in the value iteration section. Printing some extra values, nothing special.\n",
    "- Removed the hyper-tuning cells from the end of the notebook.\n",
    "- Found the bug in the `QLearningAgent` class, and corrected that.\n",
    "- The `QLearningAgent` class and the `run_experiment` function are re-defined in terms of code, i.e., the agent's class is more inclusive now in terms of the capabilities.\n",
    "- Another key change in this version is that the step-size decreases over the iterations, in Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from itertools import product\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145a9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CODE\n",
    "# Setting the seed for reproducible results\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb3a9e",
   "metadata": {},
   "source": [
    "# 1. Environment\n",
    "- The below code cell provides the backbone of the `ExampleEnvironment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e40d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnvironment():\n",
    "    def __init__(self, env_info={}):\n",
    "        # These are the different possible states\n",
    "        self.grid = [0, 1, 2, 3]\n",
    "        \n",
    "        # The rewards produced by the environment in response to the different ...\n",
    "        # ... actions of the agent in different states\n",
    "        self.rewards = [\n",
    "            [0, 0, 2],\n",
    "            [0, 1, 0],\n",
    "            [1, 1, 0],\n",
    "            [2, 1.5, 3]\n",
    "        ]\n",
    "\n",
    "        # The environment is governed by the following dynamics\n",
    "        # In mathematical notation, this is nothing but p(s'|s,a)\n",
    "        # But in this example, we are assuming to be independent of actions, i.e., ...\n",
    "        # p(s'|s, a) is equal for all actions in state s\n",
    "        self.tran_matrix = np.array([\n",
    "            [1/2, 1/2, 0, 0],    # State 0\n",
    "            [1/4, 1/4, 1/2, 0],  # State 1\n",
    "            [0, 1/4, 1/4, 1/2],  # State 2\n",
    "            [0, 0, 1/4, 3/4]     # State 3\n",
    "        ])\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(env_info.get(\"seed\", 0))\n",
    "        \n",
    "        # Defines the current location\n",
    "        self.cur_loc = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.cur_loc = self.rand_generator.choice(self.grid)\n",
    "        return self.cur_loc\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_reward = self.rewards[self.cur_loc][action]\n",
    "        next_state = self.rand_generator.choice(self.grid, \n",
    "            p = self.tran_matrix[self.cur_loc])\n",
    "        self.cur_loc = next_state\n",
    "        return next_state, next_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b406c9c",
   "metadata": {},
   "source": [
    "# 2. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1822e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(theta = 0.01, discount = 0.9):\n",
    "    # Creating an instance for the environment\n",
    "    env = ExampleEnvironment()\n",
    "\n",
    "    # Defining the paramters for the simulation\n",
    "    delta = theta * 10\n",
    "\n",
    "    # Initializing the state values and the different possible actions\n",
    "    s_vals = np.zeros(4)\n",
    "    actions = list(np.arange(3))\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in env.grid:\n",
    "            cur_val = copy.copy(s_vals[s])\n",
    "            vals = []\n",
    "            for a in actions:\n",
    "                sum_rhs = env.tran_matrix[s] * (env.rewards[s][a] + discount * s_vals)\n",
    "                vals.append(np.sum(sum_rhs))\n",
    "            s_vals[s] = np.max(vals)\n",
    "            delta = max(delta, abs(cur_val - s_vals[s]))\n",
    "            \n",
    "    return s_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49055fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Convergence of Value Iteration Algorithm\n",
      "State Values:  [18.61753617 18.31203114 20.0076184  23.08053263]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "State Values:  [ 8.74730387  8.12262406  9.37273269 12.18526228]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "State Values:  [5.71678855 4.90395976 5.84467657 8.46846691]\n"
     ]
    }
   ],
   "source": [
    "s_vals = value_iteration(theta = 0.001, discount = 0.9)\n",
    "print(\"Post Convergence of Value Iteration Algorithm\")\n",
    "print(\"State Values: \", s_vals)\n",
    "\n",
    "s_vals = value_iteration(theta = 0.001, discount = 0.8)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"State Values: \", s_vals)\n",
    "\n",
    "s_vals = value_iteration(theta = 0.001, discount = 0.7)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"State Values: \", s_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666159c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_using_q(theta = 0.01, discount = 0.9):\n",
    "    # Creating an instance for the environment\n",
    "    env = ExampleEnvironment()\n",
    "\n",
    "    # Defining the paramters for the simulation\n",
    "    delta = theta * 10\n",
    "\n",
    "    # Initializing the action values and the different possible actions\n",
    "    rand_generator = np.random.RandomState(0)\n",
    "    q_vals = rand_generator.uniform(0, 0.1, (4, 3))\n",
    "    # q_vals = np.zeros((4, 3))\n",
    "    actions = list(np.arange(3))\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in env.grid:\n",
    "            for a in actions:\n",
    "                cur_val = copy.copy(q_vals[s][a])\n",
    "                sum_rhs = 0\n",
    "                for next_s in env.grid:\n",
    "                    sum_rhs += env.tran_matrix[s][next_s] * (\n",
    "                        env.rewards[s][a] + discount * max(q_vals[next_s]) \n",
    "                    )\n",
    "                q_vals[s][a] = sum_rhs\n",
    "                delta = max(delta, abs(cur_val - q_vals[s][a]))\n",
    "\n",
    "    return q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e2033e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Convergence of Value Iteration Algorithm\n",
      "Action Values:  [[16.61795239 16.61795239 18.61795239]\n",
      " [17.31252016 18.31252016 17.31270256]\n",
      " [20.0080671  20.00820908 19.00824102]\n",
      " [22.08096246 21.58096246 23.08096246]]\n",
      "State Values:  [18.61795239 18.31252016 20.00820908 23.08096246]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "Action Values:  [[ 6.74775008  6.74775008  8.74775008]\n",
      " [ 7.12308557  8.12308557  7.12321984]\n",
      " [ 9.37314418  9.37324666  8.37326715]\n",
      " [11.1856504  10.6856504  12.1856504 ]]\n",
      "State Values:  [ 8.74775008  8.12308557  9.37324666 12.1856504 ]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "Action Values:  [[3.71658373 3.71658373 5.71658373]\n",
      " [3.90385135 4.90385135 3.90397635]\n",
      " [5.84454004 5.84463445 4.84465097]\n",
      " [7.46830996 6.96830996 8.46830996]]\n",
      "State Values:  [5.71658373 4.90385135 5.84463445 8.46830996]\n"
     ]
    }
   ],
   "source": [
    "q_vals = value_iteration_using_q(theta = 0.001, discount = 0.9)\n",
    "print(\"Post Convergence of Value Iteration Algorithm\")\n",
    "print(\"Action Values: \", q_vals)\n",
    "print(\"State Values: \", np.max(q_vals, axis = 1))\n",
    "\n",
    "q_vals = value_iteration_using_q(theta = 0.001, discount = 0.8)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"Action Values: \", q_vals)\n",
    "print(\"State Values: \", np.max(q_vals, axis = 1))\n",
    "\n",
    "q_vals = value_iteration_using_q(theta = 0.001, discount = 0.7)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"Action Values: \", q_vals)\n",
    "print(\"State Values: \", np.max(q_vals, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e4a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Action Values:  [[16.56632785 16.56632785 18.56632785]\n",
      " [17.26489864 18.26489864 17.26675177]\n",
      " [19.96135728 19.96279976 18.96312432]\n",
      " [22.03446822 21.53446822 23.03446822]]\n",
      "Optimal State Values:  [18.56632785 18.26489864 19.96279976 23.03446822]\n"
     ]
    }
   ],
   "source": [
    "optimal_q_vals = value_iteration_using_q(theta = 0.01, discount = 0.9)\n",
    "print(\"Optimal Action Values: \", optimal_q_vals)\n",
    "print(\"Optimal State Values: \", np.max(optimal_q_vals, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e33c7",
   "metadata": {},
   "source": [
    "# 3. Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c56c62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, agent_info={}):\n",
    "        # Defining the #actions and #states \n",
    "        self.num_actions = 3\n",
    "        self.num_states = 4\n",
    "        \n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\", 0.9)\n",
    "\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "        \n",
    "        # Whether the step-size is decreasing or not\n",
    "        self.is_step_decreasing = agent_info.get(\"is_step_decreasing\", False)\n",
    "        \n",
    "        if self.is_step_decreasing:\n",
    "            # The ratio by which the step-size should be multiplied\n",
    "            self.reduce_step_with = agent_info.get(\"reduce_step_with\", 0.9)\n",
    "            \n",
    "            # The number of steps after which the step-size should be reduced\n",
    "            self.reduce_step_after = agent_info.get(\"reduce_step_after\", 100)\n",
    "\n",
    "        # To control the exploration-exploitation trade-off\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        \n",
    "        # To determine if the Q-function is converged or not\n",
    "        self.delta = agent_info.get(\"delta\", 0.01)\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\", 0))\n",
    "        \n",
    "        # Definining the Optimal Q-Values to which the algorithm should converge to\n",
    "        self.optimal_q = agent_info.get(\"optimal_q\", None)\n",
    "        \n",
    "        # Defining the initial action values\n",
    "        # self.q = self.rand_generator.randn(self.num_states, self.num_actions)\n",
    "        self.q = self.rand_generator.uniform(0, 0.1, (self.num_states, self.num_actions))\n",
    "        \n",
    "        # Initializing the variables for the previous state and action\n",
    "        self.prev_state  = None\n",
    "        self.prev_action = None\n",
    "        \n",
    "        # Tracking the number of steps\n",
    "        self.cur_step = 0\n",
    "        \n",
    "    def start(self, state):\n",
    "        # Increasing the count of steps\n",
    "        self.cur_step += 1\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "            \n",
    "        # Reducing the step-size\n",
    "        if self.is_step_decreasing and self.cur_step % self.reduce_step_after == 0:\n",
    "            old_step_size = copy.copy(self.step_size)\n",
    "            self.step_size *= self.reduce_step_with\n",
    "            print(f\"At {self.cur_step} steps | Reduced step-size from {old_step_size} to {self.step_size}\")\n",
    "            \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return self.cur_step, action\n",
    "    \n",
    "    def step(self, state, reward):\n",
    "        # Increasing the count of steps\n",
    "        self.cur_step += 1\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "            \n",
    "        # Reducing the step-size\n",
    "        if self.is_step_decreasing and self.cur_step % self.reduce_step_after == 0:\n",
    "            old_step_size = copy.copy(self.step_size)\n",
    "            self.step_size *= self.reduce_step_with\n",
    "            print(f\"At {self.cur_step} steps | Reduced step-size from {old_step_size} to {self.step_size}\")\n",
    "        \n",
    "        # Determining the new Q-Value\n",
    "        new_val = -1e8\n",
    "        cur_val = copy.copy(self.q[self.prev_state, self.prev_action])\n",
    "        for act in range(self.num_actions):\n",
    "            val = cur_val + self.step_size * (\n",
    "                reward + self.discount * self.q[state, act] - cur_val\n",
    "            )\n",
    "            new_val = max(new_val, val)\n",
    "        self.q[self.prev_state, self.prev_action] = new_val\n",
    "        \n",
    "        ### DEBUGGING CODE\n",
    "        # print(\"Inside the Agent's Step method:\")\n",
    "        # print(f\"Old value: {cur_val}; New value: {new_val}\")\n",
    "        \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "            \n",
    "        # Determining if the Q-function has converged or not\n",
    "        if np.max(np.abs(self.optimal_q - self.q)) < self.delta:\n",
    "            return (self.cur_step, action, True)\n",
    "        else:\n",
    "            return (self.cur_step, action, False)\n",
    "            \n",
    "    def argmax(self, q_values):\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09924af7",
   "metadata": {},
   "source": [
    "# 4. Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d64c00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "        env_info = {}, agent_info = {}, max_iter = 1000, \n",
    "        re_init = 100, print_vals = True\n",
    "    ):\n",
    "    env = ExampleEnvironment(env_info) \n",
    "    agent = QLearningAgent(agent_info)\n",
    "    has_converged = False\n",
    "    \n",
    "    init_state  = env.start()                             # STARTING STATE\n",
    "    num_iter, init_action = agent.start(init_state)                 # STARTING ACTION\n",
    "    next_state, next_reward = env.step(init_action)       # STARTING REWARD\n",
    "    \n",
    "    while not has_converged and num_iter < max_iter:\n",
    "        ### DEBUGGING CODE\n",
    "        # print(f\"Current State: {next_state}\")\n",
    "        \n",
    "        # After every `re_init` steps, re-initialize with a random state\n",
    "        if num_iter % re_init == 0:\n",
    "            init_state  = env.start()                             \n",
    "            num_iter, init_action = agent.start(init_state)                 \n",
    "            next_state, next_reward = env.step(init_action)\n",
    "        else:\n",
    "            num_iter, next_action, has_converged = agent.step(next_state, next_reward)\n",
    "            next_state, next_reward = env.step(next_action)\n",
    "            \n",
    "        ### DEBUGGING CODE\n",
    "        # print(f\"Next Action, State & Reward: {next_action}, {next_state}, {next_reward}\")\n",
    "        # print()\n",
    "            \n",
    "        if print_vals and num_iter % (max_iter / 5) == 0:\n",
    "            print(f\"\\nTime Steps Elapsed | {num_iter}\")\n",
    "            print(\"Q-Values:\", agent.q)\n",
    "            print()\n",
    "                \n",
    "    print(\"POST CONVERGENCE\\n\")\n",
    "    print(\"Optimal Action Values:\")\n",
    "    print(agent.q)\n",
    "    \n",
    "    print(\"\\nOptimal State Values:\")\n",
    "    print(np.max(agent.q, axis = -1))\n",
    "    \n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    print(np.argmax(agent.q, axis = -1))\n",
    "    \n",
    "    return agent.q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b34cf",
   "metadata": {},
   "source": [
    "## 4.1.\n",
    "- **Step-size**: 0.1\n",
    "- **Step-size decreases**: False\n",
    "- **Max iterations**: 5000\n",
    "- **Re-initialization with random state**: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae04f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the characteristics for the environment\n",
    "env_info = {\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 0.9,       \n",
    "    \"step_size\": 0.1,\n",
    "    \"is_step_decreasing\": False,\n",
    "    \"epsilon\": 0.2,\n",
    "    \"delta\": 1e-2,\n",
    "    \"optimal_q\": optimal_q_vals,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "max_iter = 5000\n",
    "re_init = max_iter+1\n",
    "\n",
    "# q_vals = run_experiment(\n",
    "#     env_info, agent_info, max_iter = max_iter, re_init = re_init,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623b7bd",
   "metadata": {},
   "source": [
    "Time Steps Elapsed | 1000 \\\n",
    "Q-Values: [[ 5.91706537  1.77898375  1.16728796]\n",
    " [ 2.9843841   9.45969105  3.04991981]\n",
    " [ 8.59734926 13.33278492  7.48044024]\n",
    " [13.1486893  13.35119285 17.61380035]]\n",
    "\n",
    "Time Steps Elapsed | 2000 \\\n",
    "Q-Values: [[10.63084301  6.25211951  5.22304306]\n",
    " [ 8.1135973  13.83607105  9.63915631]\n",
    " [14.81208161 19.51124745 16.26131564]\n",
    " [19.11579253 20.10431106 22.08680841]]\n",
    "\n",
    "Time Steps Elapsed | 3000 \\\n",
    "Q-Values: [[12.96019131  8.10097292  6.04424472]\n",
    " [13.4106704  16.21106108 12.87251928]\n",
    " [19.37717581 17.94164608 17.77797443]\n",
    " [21.52348462 20.84242585 22.35550366]]\n",
    "\n",
    "Time Steps Elapsed | 4000 \\\n",
    "Q-Values: [[13.33265381  9.2731831  10.89189185]\n",
    " [13.74486253 17.11685695 14.41056284]\n",
    " [20.23009473 18.62739035 18.37479771]\n",
    " [21.89894691 21.32149142 23.5644888 ]]\n",
    "\n",
    "Time Steps Elapsed | 5000 \\\n",
    "Q-Values: [[13.82051964 10.61230837 12.1141355 ]\n",
    " [15.32899705 14.94439797 14.92686395]\n",
    " [19.10309399 18.13843012 18.26552303]\n",
    " [21.22485671 20.80919132 22.16413601]]\n",
    "\n",
    "**POST CONVERGENCE**\n",
    "\n",
    "Optimal Action Values: \\\n",
    "[[13.82051964 10.61230837 12.1141355 ]\n",
    " [15.32899705 14.94439797 14.92686395]\n",
    " [19.10309399 18.13843012 18.26552303]\n",
    " [21.22485671 20.80919132 22.16413601]]\n",
    "\n",
    "Optimal State Values: \\\n",
    "[13.82051964 15.32899705 19.10309399 22.16413601]\n",
    "\n",
    "Optimal Policy: \\\n",
    "[0 0 0 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c90a74",
   "metadata": {},
   "source": [
    "## 4.2.\n",
    "- **Step-size**: 0.18\n",
    "- **Step-size decreases**: True\n",
    "- **Step-size decreases after (#iterations)**: 2000\n",
    "- **Step-size reduces to (ratio of step-size)**: 0.9\n",
    "- **Max iterations**: 40000\n",
    "- **Re-initialization with random state**: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab53cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the characteristics for the environment\n",
    "env_info = {\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 0.9,       \n",
    "    \"step_size\": 0.18,\n",
    "    \"is_step_decreasing\": True,\n",
    "    \"reduce_step_after\": 2000,\n",
    "    \"reduce_step_with\": 0.9,\n",
    "    \"epsilon\": 0.2,\n",
    "    \"delta\": 1e-2,\n",
    "    \"optimal_q\": optimal_q_vals,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "max_iter = 40000\n",
    "re_init = max_iter+1\n",
    "\n",
    "# q_vals = run_experiment(\n",
    "#     env_info, agent_info, max_iter = max_iter, re_init = re_init,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2da01",
   "metadata": {},
   "source": [
    "At 2000 steps | Reduced step-size from 0.18 to 0.162 \\\n",
    "At 4000 steps | Reduced step-size from 0.162 to 0.1458 \\\n",
    "At 6000 steps | Reduced step-size from 0.1458 to 0.13122 \\\n",
    "At 8000 steps | Reduced step-size from 0.13122 to 0.11809800000000001\n",
    "\n",
    "Time Steps Elapsed | 8000 \\\n",
    "Q-Values: [[15.76237922 15.80090396 18.61897938]\n",
    " [17.45979802 18.75160507 16.72741991]\n",
    " [20.4326306  20.65684392 19.46503777]\n",
    " [22.52664068 22.07555163 23.71391028]]\n",
    "\n",
    "At 10000 steps | Reduced step-size from 0.11809800000000001 to 0.10628820000000001 \\\n",
    "At 12000 steps | Reduced step-size from 0.10628820000000001 to 0.09565938000000002 \\\n",
    "At 14000 steps | Reduced step-size from 0.09565938000000002 to 0.08609344200000002 \\\n",
    "At 16000 steps | Reduced step-size from 0.08609344200000002 to 0.07748409780000003\n",
    "\n",
    "Time Steps Elapsed | 16000 \\\n",
    "Q-Values: [[16.33016865 16.37623803 18.41111757]\n",
    " [16.89598341 17.77288431 16.99602485]\n",
    " [18.66978249 19.48824204 18.15942152]\n",
    " [21.27080866 21.28225674 22.11279888]]\n",
    "\n",
    "At 18000 steps | Reduced step-size from 0.07748409780000003 to 0.06973568802000002 \\\n",
    "At 20000 steps | Reduced step-size from 0.06973568802000002 to 0.06276211921800003 \\\n",
    "At 22000 steps | Reduced step-size from 0.06276211921800003 to 0.056485907296200025 \\\n",
    "At 24000 steps | Reduced step-size from 0.056485907296200025 to 0.050837316566580026\n",
    "\n",
    "Time Steps Elapsed | 24000 \\\n",
    "Q-Values: [[16.42076951 16.46958468 18.62670041]\n",
    " [17.35213046 18.66656052 17.13923243]\n",
    " [19.73796572 20.88258005 18.5753902 ]\n",
    " [22.6976442  21.90123185 23.90575936]]\n",
    "\n",
    "At 26000 steps | Reduced step-size from 0.050837316566580026 to 0.04575358490992203 \\\n",
    "At 28000 steps | Reduced step-size from 0.04575358490992203 to 0.04117822641892983 \\ \n",
    "At 30000 steps | Reduced step-size from 0.04117822641892983 to 0.03706040377703684 \\ \n",
    "At 32000 steps | Reduced step-size from 0.03706040377703684 to 0.03335436339933316\n",
    "\n",
    "Time Steps Elapsed | 32000 \\\n",
    "Q-Values: [[16.6435579  16.61035947 18.78110653]\n",
    " [17.3123905  18.46059651 17.41002517]\n",
    " [20.04673234 19.53399665 18.79399951]\n",
    " [22.39480771 21.77848655 22.95162231]]\n",
    "\n",
    "At 34000 steps | Reduced step-size from 0.03335436339933316 to 0.030018927059399847 \\\n",
    "At 36000 steps | Reduced step-size from 0.030018927059399847 to 0.027017034353459864 \\\n",
    "At 38000 steps | Reduced step-size from 0.027017034353459864 to 0.02431533091811388 \\\n",
    "At 40000 steps | Reduced step-size from 0.02431533091811388 to 0.02188379782630249\n",
    "\n",
    "Time Steps Elapsed | 40000 \\\n",
    "Q-Values: [[16.59362476 16.5602848  18.67161439]\n",
    " [17.2721398  18.53197893 17.31961091]\n",
    " [19.96192094 19.82840197 19.30500788]\n",
    " [22.33773805 21.81566305 23.20383915]]\n",
    "\n",
    "**POST CONVERGENCE**\n",
    "\n",
    "Optimal Action Values: \\\n",
    "[[16.59362476 16.5602848  18.67161439]\n",
    " [17.2721398  18.53197893 17.31961091]\n",
    " [19.96192094 19.82840197 19.30500788]\n",
    " [22.33773805 21.81566305 23.20383915]]\n",
    "\n",
    "Optimal State Values: \\\n",
    "[18.67161439 18.53197893 19.96192094 23.20383915]\n",
    "\n",
    "Optimal Policy: \\\n",
    "[2 1 0 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
