{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbbae57",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- In this kernel, we will be implementing an example environment.\n",
    "- We will be deploying SARSA, Q-Learning and Expected SARSA to try and find the optimal agent's policy and the optimal value functions, in order to maximize the rewards.\n",
    "\n",
    "# Importing Packages & Boilerplate Stuff\n",
    "\n",
    "1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
    "2. numpy: the fundamental package for scientific computing with Python.\n",
    "3. matplotlib: the library for plotting graphs in Python.\n",
    "4. RL-Glue: the library for reinforcement learning experiments.\n",
    "5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.\n",
    "6. itertools.product: the function that can be used easily to compute permutations.\n",
    "7. tqdm.tqdm: Provides progress bars for visualizing the status of loops.\n",
    "\n",
    "# Based on Template (Changes)\n",
    "- In this version, I will transform the value iteration algorithm to use the action values instead of the state values.\n",
    "- Also, I will run the Q-Learning Algorithm till it reaches either a maximum number of iterations or converges to the Q-values computed by the value iteration algorithm.\n",
    "- Additionally, after a certain number of steps, I have re-initialized the agent from a random state, periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from itertools import product\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145a9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CODE\n",
    "# Setting the seed for reproducible results\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb3a9e",
   "metadata": {},
   "source": [
    "# 1. Environment\n",
    "- The below code cell provides the backbone of the `ExampleEnvironment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e40d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnvironment():\n",
    "    def __init__(self, env_info={}):\n",
    "        # These are the different possible states\n",
    "        self.grid = [0, 1, 2, 3]\n",
    "        \n",
    "        # The rewards produced by the environment in response to the different ...\n",
    "        # ... actions of the agent in different states\n",
    "        self.rewards = [\n",
    "            [0, 0, 2],\n",
    "            [0, 1, 0],\n",
    "            [1, 1, 0],\n",
    "            [2, 1.5, 3]\n",
    "        ]\n",
    "\n",
    "        # The environment is governed by the following dynamics\n",
    "        # In mathematical notation, this is nothing but p(s'|s,a)\n",
    "        # But in this example, we are assuming to be independent of actions, i.e., ...\n",
    "        # p(s'|s, a) is equal for all actions in state s\n",
    "        self.tran_matrix = np.array([\n",
    "            [1/2, 1/2, 0, 0],    # State 0\n",
    "            [1/4, 1/4, 1/2, 0],  # State 1\n",
    "            [0, 1/4, 1/4, 1/2],  # State 2\n",
    "            [0, 0, 1/4, 3/4]     # State 3\n",
    "        ])\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(env_info.get(\"seed\", 0))\n",
    "        \n",
    "        # Defines the current location\n",
    "        self.cur_loc = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.cur_loc = self.rand_generator.choice(self.grid)\n",
    "        return self.cur_loc\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_reward = self.rewards[self.cur_loc][action]\n",
    "        next_state = self.rand_generator.choice(self.grid, \n",
    "            p = self.tran_matrix[self.cur_loc])\n",
    "        self.cur_loc = next_state\n",
    "        return next_state, next_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b406c9c",
   "metadata": {},
   "source": [
    "# 2. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1822e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(theta = 0.01, discount = 0.9):\n",
    "    # Creating an instance for the environment\n",
    "    env = ExampleEnvironment()\n",
    "\n",
    "    # Defining the paramters for the simulation\n",
    "    delta = theta * 10\n",
    "\n",
    "    # Initializing the state values and the different possible actions\n",
    "    s_vals = np.zeros(4)\n",
    "    actions = list(np.arange(3))\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in env.grid:\n",
    "            cur_val = copy.copy(s_vals[s])\n",
    "            vals = []\n",
    "            for a in actions:\n",
    "                sum_rhs = env.tran_matrix[s] * (env.rewards[s][a] + discount * s_vals)\n",
    "                vals.append(np.sum(sum_rhs))\n",
    "            s_vals[s] = np.max(vals)\n",
    "            delta = max(delta, abs(cur_val - s_vals[s]))\n",
    "            \n",
    "    return s_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dd0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_using_q(theta = 0.01, discount = 0.9):\n",
    "    # Creating an instance for the environment\n",
    "    env = ExampleEnvironment()\n",
    "\n",
    "    # Defining the paramters for the simulation\n",
    "    delta = theta * 10\n",
    "\n",
    "    # Initializing the action values and the different possible actions\n",
    "    rand_generator = np.random.RandomState(0)\n",
    "    q_vals = rand_generator.uniform(0, 0.1, (4, 3))\n",
    "    # q_vals = np.zeros((4, 3))\n",
    "    actions = list(np.arange(3))\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in env.grid:\n",
    "            for a in actions:\n",
    "                cur_val = copy.copy(q_vals[s][a])\n",
    "                sum_rhs = 0\n",
    "                for next_s in env.grid:\n",
    "                    sum_rhs += env.tran_matrix[s][next_s] * (\n",
    "                        env.rewards[s][a] + discount * max(q_vals[next_s]) \n",
    "                    )\n",
    "                q_vals[s][a] = sum_rhs\n",
    "                delta = max(delta, abs(cur_val - q_vals[s][a]))\n",
    "\n",
    "    return q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e2033e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Convergence of Value Iteration Algorithm\n",
      "Action Values:  [[16.56632785 16.56632785 18.56632785]\n",
      " [17.26489864 18.26489864 17.26675177]\n",
      " [19.96135728 19.96279976 18.96312432]\n",
      " [22.03446822 21.53446822 23.03446822]]\n",
      "State Values:  [18.56632785 18.26489864 19.96279976 23.03446822]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "Action Values:  [[ 6.72506248  6.72506248  8.72506248]\n",
      " [ 7.10378825  8.10378825  7.10527654]\n",
      " [ 9.35444219  9.35557743  8.35580448]\n",
      " [11.1670125  10.6670125  12.1670125 ]]\n",
      "State Values:  [ 8.72506248  8.10378825  9.35557743 12.1670125 ]\n",
      "\n",
      "Post Convergence of Value Iteration Algorithm\n",
      "Action Values:  [[3.70214915 3.70214915 5.70214915]\n",
      " [3.89254525 4.89254525 3.89385972]\n",
      " [5.83362341 5.8346134  4.83478665]\n",
      " [7.45735818 6.95735818 8.45735818]]\n",
      "State Values:  [5.70214915 4.89254525 5.8346134  8.45735818]\n"
     ]
    }
   ],
   "source": [
    "q_vals = value_iteration_using_q(theta = 0.01, discount = 0.9)\n",
    "print(\"Post Convergence of Value Iteration Algorithm\")\n",
    "print(\"Action Values: \", q_vals)\n",
    "print(\"State Values: \", np.max(q_vals, axis = 1))\n",
    "\n",
    "q_vals = value_iteration_using_q(theta = 0.01, discount = 0.8)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"Action Values: \", q_vals)\n",
    "print(\"State Values: \", np.max(q_vals, axis = 1))\n",
    "\n",
    "q_vals = value_iteration_using_q(theta = 0.01, discount = 0.7)\n",
    "print(\"\\nPost Convergence of Value Iteration Algorithm\")\n",
    "print(\"Action Values: \", q_vals)\n",
    "print(\"State Values: \", np.max(q_vals, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de09e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_q_vals = value_iteration_using_q(theta = 0.01, discount = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e33c7",
   "metadata": {},
   "source": [
    "# 3. Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56c62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, agent_info={}):\n",
    "        # Defining the #actions and #states \n",
    "        self.num_actions = 3\n",
    "        self.num_states = 4\n",
    "        \n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\", 0.9)\n",
    "\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "\n",
    "        # To control the exploration-exploitation trade-off\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        \n",
    "        # To determine if the Q-function is converged or not\n",
    "        self.delta = agent_info.get(\"delta\", 0.01)\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\", 0))\n",
    "        \n",
    "        # Definining the Optimal Q-Values to which the algorithm should converge to\n",
    "        self.optimal_q = agent_info.get(\"optimal_q\", None)\n",
    "        \n",
    "        # Defining the initial action values\n",
    "        # self.q = self.rand_generator.randn(self.num_states, self.num_actions)\n",
    "        self.q = self.rand_generator.uniform(0, 0.1, (self.num_states, self.num_actions))\n",
    "        \n",
    "        # Initializing the variables for the previous state and action\n",
    "        self.prev_state  = None\n",
    "        self.prev_action = None\n",
    "        \n",
    "    def start(self, state):\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "            \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def step(self, state, reward):\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        # Determining the new Q-Value\n",
    "        new_val = -1e8\n",
    "        cur_val = copy.copy(self.q[self.prev_state, self.prev_action])\n",
    "        for act in range(self.num_actions):\n",
    "            val = cur_val + self.step_size * (\n",
    "                reward + self.discount * self.q[state, act] - cur_val\n",
    "            )\n",
    "            new_val = max(new_val, val)\n",
    "        self.q[self.prev_state, self.prev_action] = new_val\n",
    "            \n",
    "        # Determining if the Q-function has converged or not\n",
    "        if np.max(np.abs(self.optimal_q - self.q)) < self.delta:\n",
    "            return (action, True)\n",
    "        else:\n",
    "            return (action, False)\n",
    "            \n",
    "    def argmax(self, q_values):\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09924af7",
   "metadata": {},
   "source": [
    "# 4. Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64c00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "        env_info = {}, agent_info = {}, max_iter = 1000, \n",
    "        re_init = 100, print_vals = True\n",
    "    ):\n",
    "    env = ExampleEnvironment(env_info) \n",
    "    agent = QLearningAgent(agent_info)\n",
    "    has_converged = False\n",
    "    num_iter = 0\n",
    "    \n",
    "    init_state  = env.start()                             # STARTING STATE\n",
    "    init_action = agent.start(init_state)                 # STARTING ACTION\n",
    "    next_state, next_reward = env.step(init_action)       # STARTING REWARD\n",
    "    num_iter = 1\n",
    "    \n",
    "    while not has_converged and num_iter < max_iter:\n",
    "        # After every `re_init` steps, re-initialize with a random state\n",
    "        if num_iter % re_init == 0:\n",
    "            init_state  = env.start()                             \n",
    "            init_action = agent.start(init_state)                 \n",
    "            next_state, next_reward = env.step(init_action)\n",
    "        else:\n",
    "            next_action, has_converged = agent.step(next_state, next_reward)\n",
    "            next_state, next_reward = env.step(next_action)\n",
    "        \n",
    "        if print_vals and num_iter % (max_iter / 5) == 0:\n",
    "            print(f\"Time Steps Elapsed | {num_iter}\")\n",
    "            print(\"Q-Values:\", agent.q)\n",
    "            print()\n",
    "        \n",
    "        num_iter += 1\n",
    "        \n",
    "    print(\"POST CONVERGENCE\\n\")\n",
    "    print(\"Optimal Action Values:\")\n",
    "    print(agent.q)\n",
    "    \n",
    "    print(\"\\nOptimal State Values:\")\n",
    "    print(np.max(agent.q, axis = -1))\n",
    "    \n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    print(np.argmax(agent.q, axis = -1))\n",
    "    \n",
    "    return agent.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae04f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Steps Elapsed | 40000\n",
      "Q-Values: [[14.78578469 10.74106156 14.74858806]\n",
      " [ 0.05448832 15.02866629  8.4647352 ]\n",
      " [14.78884553 15.55928784 14.67842726]\n",
      " [ 0.03834415 15.03340622 13.9267657 ]]\n",
      "\n",
      "Time Steps Elapsed | 80000\n",
      "Q-Values: [[14.4954237  13.91199113 15.50748304]\n",
      " [13.66807433 12.77897123 15.43329383]\n",
      " [14.58691495 13.28005356 14.72572957]\n",
      " [14.43598089 14.33022159 14.6201526 ]]\n",
      "\n",
      "Time Steps Elapsed | 120000\n",
      "Q-Values: [[14.4954237  15.06452477 17.4522008 ]\n",
      " [15.80416773 15.49463439 14.78653962]\n",
      " [17.65758831 15.2211712  17.71491988]\n",
      " [15.40381349 14.33022159 18.70143609]]\n",
      "\n",
      "Time Steps Elapsed | 160000\n",
      "Q-Values: [[14.4954237  16.63223089 17.15558977]\n",
      " [18.4343816  17.67180333 16.83260701]\n",
      " [16.93421128 15.2211712  17.34455392]\n",
      " [17.85977507 18.25331301 18.1708119 ]]\n",
      "\n",
      "POST CONVERGENCE\n",
      "\n",
      "Optimal Action Values:\n",
      "[[14.4954237  17.3281502  16.69441025]\n",
      " [16.5784202  17.1556466  17.50481657]\n",
      " [17.69424827 17.16945316 16.41097285]\n",
      " [16.77760638 17.68499115 17.17074581]]\n",
      "\n",
      "Optimal State Values:\n",
      "[17.3281502  17.50481657 17.69424827 17.68499115]\n",
      "\n",
      "Optimal Policy:\n",
      "[1 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Defining the characteristics for the environment\n",
    "env_info = {\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 0.9,       \n",
    "    \"step_size\": 0.2,\n",
    "    \"epsilon\": 0.2,\n",
    "    \"delta\": 1e-3,\n",
    "    \"optimal_q\": optimal_q_vals,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "q_vals = run_experiment(env_info, agent_info, max_iter = 200000, re_init = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc69e7e",
   "metadata": {},
   "source": [
    "## 4.1. Trying to understand the effect of step-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d44066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step_sizes = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# # Defining the characteristics for the environment\n",
    "# env_info = {\n",
    "#     \"seed\": 0\n",
    "# }\n",
    "\n",
    "# # Defining the characteristics for the agent\n",
    "# agent_info = {\n",
    "#     \"discount\": 0.9,       \n",
    "#     \"step_size\": 0.1,\n",
    "#     \"epsilon\": 0.1,\n",
    "#     \"delta\": 1e-2,\n",
    "#     \"seed\": 0\n",
    "# }\n",
    "\n",
    "# state_vals_ss = []\n",
    "\n",
    "# for ss in step_sizes:\n",
    "#     print(\"\\n\\nFor step-size:\", ss)\n",
    "#     agent_info['step_size'] = ss\n",
    "#     q_vals = run_experiment(env_info, agent_info, print_vals = False)\n",
    "#     state_vals = np.max(q_vals, axis = -1)\n",
    "#     state_vals_ss.append(state_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a535654c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (10, 6))\n",
    "\n",
    "# for i in range(len(step_sizes)):\n",
    "#     plt.plot(state_vals_ss[i], label = f'Step-Size = {step_sizes[i]}')\n",
    "    \n",
    "# plt.xticks(np.arange(4))\n",
    "# plt.title(\"Optimal State Values for different step-size\")\n",
    "# plt.xlabel(\"States\")\n",
    "# plt.ylabel(\"Optimal State Values\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b080558",
   "metadata": {},
   "source": [
    "## 4.2. Trying to understand the effect of epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e83cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Trying to understand the effect of step-size\n",
    "# eps = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "# # Defining the characteristics for the environment\n",
    "# env_info = {\n",
    "#     \"seed\": 0\n",
    "# }\n",
    "\n",
    "# # Defining the characteristics for the agent\n",
    "# agent_info = {\n",
    "#     \"discount\": 0.9,       \n",
    "#     \"step_size\": 0.1,\n",
    "#     \"epsilon\": 0.1,\n",
    "#     \"delta\": 1e-2,\n",
    "#     \"seed\": 0\n",
    "# }\n",
    "\n",
    "# state_vals_eps = []\n",
    "\n",
    "# for e in eps:\n",
    "#     print(\"\\n\\nFor epsilon:\", e)\n",
    "#     agent_info['epsilon'] = e\n",
    "#     q_vals = run_experiment(env_info, agent_info, print_vals = False)\n",
    "#     state_vals = np.max(q_vals, axis = -1)\n",
    "#     state_vals_eps.append(state_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da844a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (10, 6))\n",
    "\n",
    "# for i in range(len(eps)):\n",
    "#     plt.plot(state_vals_eps[i], label = f'Epsilon = {eps[i]}')\n",
    "    \n",
    "# plt.xticks(np.arange(4))\n",
    "# plt.title(\"Optimal State Values for different epsilon\")\n",
    "# plt.xlabel(\"States\")\n",
    "# plt.ylabel(\"Optimal State Values\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
