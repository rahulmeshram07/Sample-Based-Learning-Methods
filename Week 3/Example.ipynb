{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbbae57",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- In this kernel, we will be implementing an example environment.\n",
    "- We will be deploying SARSA, Q-Learning and Expected SARSA to try and find the optimal agent's policy and the optimal value functions, in order to maximize the rewards.\n",
    "\n",
    "# Importing Packages & Boilerplate Stuff\n",
    "\n",
    "1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
    "2. numpy: the fundamental package for scientific computing with Python.\n",
    "3. matplotlib: the library for plotting graphs in Python.\n",
    "4. RL-Glue: the library for reinforcement learning experiments.\n",
    "5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.\n",
    "6. itertools.product: the function that can be used easily to compute permutations.\n",
    "7. tqdm.tqdm: Provides progress bars for visualizing the status of loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from itertools import product\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145a9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CODE\n",
    "# Setting the seed for reproducible results\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb3a9e",
   "metadata": {},
   "source": [
    "# 1. Environment\n",
    "- The below code cell provides the backbone of the `ExampleEnvironment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e40d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnvironment():\n",
    "    def __init__(self, env_info={}):\n",
    "        # These are the different possible states\n",
    "        self.grid = [0, 1, 2, 3]\n",
    "        \n",
    "        # These are the different possible actions\n",
    "        # 0 -> LEFT, 1 -> RIGHT, 2 -> STAY \n",
    "        # In states 0 and 3, LEFT and RIGHT actions will lead to the same state as STAY\n",
    "        self.tran_matrix = [\n",
    "            [1/4, 1/2, 1/4], \n",
    "            [1/4, 1/2, 1/4],\n",
    "            [1/4, 1/2, 1/4],\n",
    "            [1/4, 1/2, 1/4]\n",
    "        ]\n",
    "        \n",
    "        # These are the rewards\n",
    "        self.rewards = [\n",
    "            [0, 0, 2],\n",
    "            [0, 1, 0],\n",
    "            [1, 1, 0],\n",
    "            [2, 1.5, 3]\n",
    "        ]\n",
    "        \n",
    "        # Defines the starting location and the current location\n",
    "        self.start_loc = 0\n",
    "        self.cur_loc = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.cur_loc = self.start_loc\n",
    "        return self.cur_loc\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = self.rewards[self.cur_loc][action]\n",
    "        \n",
    "        if action == 0:\n",
    "            self.cur_loc = max(0, self.cur_loc - 1)\n",
    "        elif action == 1:\n",
    "            self.cur_loc = min(3, self.cur_loc + 1)\n",
    "        elif action == 2:\n",
    "            pass\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e33c7",
   "metadata": {},
   "source": [
    "# 2. Learning Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56c62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, agent_info={}):\n",
    "        # Defining the #actions and #states \n",
    "        self.num_actions = 3\n",
    "        self.num_states = 4\n",
    "        \n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\", 0.9)\n",
    "\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "\n",
    "        # To control the exploration-exploitation trade-off\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        \n",
    "        # To determine if the Q-function is converged or not\n",
    "        self.delta = agent_info.get(\"delta\", 0.01)\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\", 0))\n",
    "        \n",
    "        # Defining the initial action values\n",
    "        self.q = self.rand_generator.randn(self.num_states, self.num_actions)\n",
    "        \n",
    "        # Initializing the variables for the previous state and action\n",
    "        self.prev_state  = None\n",
    "        self.prev_action = None\n",
    "        \n",
    "    def start(self, state):\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "            \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def step(self, state, reward):\n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        # Determining the new Q-Value\n",
    "        new_val = -1e8\n",
    "        cur_val = copy.copy(self.q[self.prev_state, self.prev_action])\n",
    "        for act in range(self.num_actions):\n",
    "            val = cur_val + self.step_size * (\n",
    "                reward + self.discount * self.q[state, act] - cur_val\n",
    "            )\n",
    "            new_val = max(new_val, val)\n",
    "        self.q[self.prev_state, self.prev_action] = new_val\n",
    "            \n",
    "        # Determining if the Q-function has converged or not\n",
    "        if abs(new_val - cur_val) < self.delta:\n",
    "            return (action, True)\n",
    "        else:\n",
    "            return (action, False)\n",
    "            \n",
    "            \n",
    "    def argmax(self, q_values):\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09924af7",
   "metadata": {},
   "source": [
    "# 3. Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64c00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env_info = {}, agent_info = {}):\n",
    "    env = ExampleEnvironment() \n",
    "    agent = QLearningAgent(agent_info)\n",
    "    has_converged = False\n",
    "    num_steps = 0\n",
    "    \n",
    "    next_state  = env.start()                 # STARTING STATE\n",
    "    next_action = agent.start(next_state)     # STARTING ACTION\n",
    "    next_reward = env.step(next_action)       # STARTING REWARD\n",
    "    \n",
    "    while not has_converged:\n",
    "        next_action, has_converged = agent.step(next_state, next_reward)\n",
    "        next_reward = env.step(next_action)\n",
    "        \n",
    "        if num_steps % 1000 == 0:\n",
    "            print(f\"Time Steps Elapsed | {num_steps}\")\n",
    "            print(\"Q-Values:\", agent.q)\n",
    "            print()\n",
    "        \n",
    "        num_steps += 1\n",
    "        \n",
    "    print(\"POST CONVERGENCE\\n\")\n",
    "    print(\"Optimal Action Values:\")\n",
    "    print(agent.q)\n",
    "    \n",
    "    print(\"\\nOptimal State Values:\")\n",
    "    print(np.max(agent.q, axis = -1))\n",
    "    \n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    print(np.argmax(agent.q, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae04f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Steps Elapsed | 0\n",
      "Q-Values: [[ 1.74641182  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 1000\n",
      "Q-Values: [[ 1.6418421   0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 2000\n",
      "Q-Values: [[ 1.53460202  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 3000\n",
      "Q-Values: [[ 1.32663728  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 4000\n",
      "Q-Values: [[ 1.21956914  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 5000\n",
      "Q-Values: [[ 1.26217655  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 6000\n",
      "Q-Values: [[ 1.39227086  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 7000\n",
      "Q-Values: [[ 0.98366648  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 8000\n",
      "Q-Values: [[ 1.32625508  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 9000\n",
      "Q-Values: [[ 1.29441687  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 10000\n",
      "Q-Values: [[ 1.51839301  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 11000\n",
      "Q-Values: [[ 1.14475024  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 12000\n",
      "Q-Values: [[ 1.36914844  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 13000\n",
      "Q-Values: [[ 1.727228    0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 14000\n",
      "Q-Values: [[ 1.15418055  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 15000\n",
      "Q-Values: [[ 1.23174995  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 16000\n",
      "Q-Values: [[ 1.07073192  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Time Steps Elapsed | 17000\n",
      "Q-Values: [[ 1.25487463  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "POST CONVERGENCE\n",
      "\n",
      "Optimal Action Values:\n",
      "[[ 0.88174024  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]]\n",
      "\n",
      "Optimal State Values:\n",
      "[0.97873798 2.2408932  0.95008842 1.45427351]\n",
      "\n",
      "Optimal Policy:\n",
      "[2 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 0.9,       \n",
    "    \"step_size\": 0.1,\n",
    "    \"epsilon\": 0.1,\n",
    "    \"delta\": 1e-4,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "run_experiment(agent_info = agent_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
