{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbbae57",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- In this kernel, we will be implementing the **Windy Gridworld** environment. A complete and extensive description of the same can be found in the book on **Reinforcement Learning** by **Richard S. Sutton and Andrew G. Barto**. Check out **Example 6.5** (Page - 130).\n",
    "- We will be deploying SARSA, Q-Learning and Expected SARSA to try and find the optimal agent's policy and the optimal value functions, in order to maximize the rewards.\n",
    "- This implementation is based on **RL-Glue**, a standard, language-independent software package for reinforcement-learning experiments. The standardization provided by RL-Glue facilitates code sharing and collaboration. Feel free to read more about it [here](https://jmlr.csail.mit.edu/papers/v10/tanner09a.html), and I borrowed the implementation from [here](https://github.com/andnp/coursera-rl-glue).\n",
    "\n",
    "# Importing Packages & Boilerplate Stuff\n",
    "\n",
    "1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
    "2. numpy: the fundamental package for scientific computing with Python.\n",
    "3. matplotlib: the library for plotting graphs in Python.\n",
    "4. RL-Glue: the library for reinforcement learning experiments.\n",
    "5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.\n",
    "6. itertools.product: the function that can be used easily to compute permutations.\n",
    "7. tqdm.tqdm: Provides progress bars for visualizing the status of loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from rl_glue import RLGlue\n",
    "from Agent import BaseAgent \n",
    "from Environment import BaseEnvironment\n",
    "from itertools import product\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CODE\n",
    "# Setting the seed for reproducible results\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb3a9e",
   "metadata": {},
   "source": [
    "# 1. Environment\n",
    "- The below code cell provides the backbone of the `WindyGridEnvironment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e40d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGridEnvironment(BaseEnvironment):\n",
    "    def env_init(self, env_info={}):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_start(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_step(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_cleanup(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # Helper method\n",
    "    def helper_method(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9123afc",
   "metadata": {},
   "source": [
    "## 1.1. env_init()\n",
    "- All the characteristics of the environment that are constant throughout the episodes are defined in this method (*i.e., need to be defined only once, in the starting of the experiment*).\n",
    "- Remember `env_init` will only be called in the beginning of the experiment, not in the beginning of each episode. So, if some attributes need to be defined in the beginning of each episode, set them in `env_start`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyGridEnvironment\n",
    "\n",
    "def env_init(self, env_info={}):\n",
    "    \"\"\"\n",
    "    Setup for the environment called when the experiment first starts.\n",
    "    Args:\n",
    "        env_info: Contains the information for the environment\n",
    "    \"\"\"\n",
    "    \n",
    "    ## NOTE: The indexing of the gridworld begins from the top-left corner\n",
    "    \n",
    "    # Define the windy grid-world; this indicates the wind in each of the columns\n",
    "    self.grid_world = np.zeros((7, 10), dtype = 'int8')\n",
    "    self.grid_world[ : , 3:6] = 1\n",
    "    self.grid_world[ : , 6:8] = 2\n",
    "    self.grid_world[ : , 8]   = 1\n",
    "    \n",
    "    # Define the starting and goal locations of agent in each episode\n",
    "    self.start_loc = [3, 0]\n",
    "    self.goal_loc  = [3, 7]\n",
    "    \n",
    "    # Define the reward for the grid-world\n",
    "    self.reward = -1\n",
    "    \n",
    "    # Define the current location of the agent; changes with each step\n",
    "    self.cur_loc = None\n",
    "    \n",
    "    # Define the reward-state-term; changes with each step\n",
    "    self.reward_state_term = None\n",
    "    \n",
    "    # Define the number of time steps elapsed in a single episode\n",
    "    self.number_of_steps = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d71fb",
   "metadata": {},
   "source": [
    "## 1.2. env_start()\n",
    "- All the characteristics of the environment that are varying across the episodes are defined in this method.\n",
    "- This includes characteristics like `dealer_card_up`, `dealer_card_down`, `agent_sum`, `agent_usable_ace` and `is_natural`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyGridEnvironment\n",
    "\n",
    "def env_start(self):\n",
    "    \"\"\"\n",
    "    The first method called when the episode starts, called before the\n",
    "    agent starts.\n",
    "    Returns:\n",
    "        The first state from the environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the initial state of the agent\n",
    "    self.cur_loc = self.start_loc\n",
    "    \n",
    "    # Defining the initial reward_state_term\n",
    "    self.reward_state_term = (0, self.cur_loc, False)\n",
    "    \n",
    "    # Define the initial number of time steps\n",
    "    self.number_of_steps = 0\n",
    "    \n",
    "    return self.reward_state_term[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60474a",
   "metadata": {},
   "source": [
    "## 1.3. env_step()\n",
    "- There are 4 possible values for the action. Let's define them here:\n",
    "    - 0: UP\n",
    "    - 1: RIGHT\n",
    "    - 2: DOWN\n",
    "    - 3: LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a33d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyGridEnvironment\n",
    "\n",
    "def env_step(self, action):\n",
    "    \"\"\"\n",
    "    A step taken by the environment.\n",
    "    Args:\n",
    "        action: The action taken by the agent\n",
    "    Returns:\n",
    "        (reward, state, boolean): a tuple of the reward, state,\n",
    "            and boolean indicating if it's terminal.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the wind strength in the current column\n",
    "    # This wind strength is used to determine the actual state due to any action ...\n",
    "    # ... taken from the present state.\n",
    "    wind = self.grid_world[self.cur_loc[0], self.cur_loc[1]]\n",
    "    \n",
    "    # The first update is the regular update\n",
    "    # The second update is due to the wind\n",
    "    # Remember that the wind only flows in the upward direction\n",
    "    \n",
    "    # UP\n",
    "    if action == 0:\n",
    "        self.cur_loc[0] = max(0, self.cur_loc[0] - 1)\n",
    "        self.cur_loc[0] = max(0, self.cur_loc[0] - wind)\n",
    "    \n",
    "    # RIGHT\n",
    "    elif action == 1:\n",
    "        self.cur_loc[1] = min(9, self.cur_loc[1] + 1)\n",
    "        self.cur_loc[0] = max(0, self.cur_loc[0] - wind)\n",
    "    \n",
    "    # DOWN\n",
    "    elif action == 2:\n",
    "        self.cur_loc[0] = min(6, self.cur_loc[0] + 1)\n",
    "        self.cur_loc[0] = max(0, self.cur_loc[0] - wind)\n",
    "    \n",
    "    # LEFT\n",
    "    elif action == 3:\n",
    "        self.cur_loc[1] = max(0, self.cur_loc[1] - 1)\n",
    "        self.cur_loc[0] = max(0, self.cur_loc[0] - wind)\n",
    "        \n",
    "    if self.cur_loc == self.goal_loc:\n",
    "        self.reward_state_term = (self.reward, self.cur_loc, True)\n",
    "    else:\n",
    "        self.reward_state_term = (self.reward, self.cur_loc, False)\n",
    "    \n",
    "    ### DEBUGGING CODE\n",
    "    self.number_of_steps += 1\n",
    "    if self.number_of_steps % 10000 == 0:\n",
    "        print(\"Number of Time Steps Elapsed:\", self.number_of_steps)\n",
    "        print(\"Current State of the Agent:\", self.cur_loc)\n",
    "    \n",
    "    return self.reward_state_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6bc14",
   "metadata": {},
   "source": [
    "## 1.4. env_cleanup()\n",
    "- This method sets the environment attributes to their default values once the experiment terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaae7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to WindyGridEnvironment\n",
    "\n",
    "def env_cleanup(self):\n",
    "    \"\"\"Cleanup done after the environment ends\"\"\"\n",
    "    \n",
    "    # Default values for the environment & the agent\n",
    "    self.grid_world = None\n",
    "    self.start_loc  = None\n",
    "    self.goal_loc   = None\n",
    "    self.reward     = None\n",
    "    self.cur_loc    = None\n",
    "    self.reward_state_term = None\n",
    "    self.number_of_steps   = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14bc18b",
   "metadata": {},
   "source": [
    "# 2. Learning Agents\n",
    "- The below code cells provides the backbone of the Learning Agents classes.\n",
    "\n",
    "### agent_init()\n",
    "- All the characteristics of the agent that are constant throughout the episodes are defined in this method.\n",
    "- The characteristics of the agent that can be modified are the **discount factor, step-size and the epsilon for exploration-exploitation trade-off**. They can be set using the `discount`, `step_size` and `epsilon` keys of the `agent_info` dictionary.\n",
    "\n",
    "### agent_start()\n",
    "- This method is called at the starting of each episode, and it receives the starting state of each episode.\n",
    "\n",
    "### agent_step()\n",
    "- This method is called throughout the episode until the agent reaches the terminal state.\n",
    "\n",
    "### agent_end()\n",
    "- This method is called when the agent reaches the terminal state, i.e., the end of an episode.\n",
    "\n",
    "### agent_message()\n",
    "- This is just a method to get some information from the agent's attributes, for instance, the Q-Values.\n",
    "\n",
    "### argmax()\n",
    "- This is just a helper method. `np.argmax` doesn't break the ties randomly, hence, we have written a custom implementation of `argmax` which does exactly that.\n",
    "\n",
    "### state_to_index() and index_to_state()\n",
    "- `state_to_index` converts the state `[int, int]` to a single index `int`, and `index_to_state` does the exact opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd11c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(BaseAgent):\n",
    "    \n",
    "    def agent_init(self, agent_info={}):\n",
    "        \"\"\"\n",
    "        Setup for the agent called when the experiment first starts.\n",
    "        Args:\n",
    "            agent_info: Contains the information for the agent\n",
    "        \"\"\"\n",
    "    \n",
    "        # Defining the #actions and #states \n",
    "        self.num_actions = 4\n",
    "        self.num_states = 70\n",
    "        \n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\", 1)\n",
    "\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "\n",
    "        # To control the exploration-exploitation trade-off\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        \n",
    "        # Defining the initial action values\n",
    "        self.q = np.zeros((self.num_states, self.num_actions))\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\", 0))\n",
    "        \n",
    "        ### DEBUGGING CODE\n",
    "        # print(self.discount, self.step_size, self.epsilon)\n",
    "    \n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"\n",
    "        The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (int): the state observation from the environment's env_start method.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert the state to an index\n",
    "        index = self.state_to_index(state)\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[index][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "            \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"\n",
    "        A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (int): the state observation from the environment's step \n",
    "                based on where the agent ended up after the last step.\n",
    "        Returns:\n",
    "            action (int): the action the agent is taking.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert the state to an index\n",
    "        index = self.state_to_index(state)\n",
    "        prev_index = self.state_to_index(self.prev_state)\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[index][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "\n",
    "        # Perform an update\n",
    "        max_val = -1e8\n",
    "        for act in range(self.num_actions):\n",
    "            val = self.q[prev_index, self.prev_action] + self.step_size * (\n",
    "                reward + self.discount * self.q[index, act] - self.q[prev_index, self.prev_action]\n",
    "            )\n",
    "            max_val = max(max_val, val)\n",
    "        self.q[prev_index, self.prev_action] = max_val\n",
    "\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"\n",
    "        Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert the state to an index\n",
    "        prev_index = self.state_to_index(self.prev_state)  \n",
    "        \n",
    "        max_val = -1e8\n",
    "        for action in range(self.num_actions):\n",
    "            val = self.q[prev_index, self.prev_action] + self.step_size * (\n",
    "                reward + self.discount * 0 - self.q[prev_index, self.prev_action]\n",
    "            )\n",
    "            max_val = max(max_val, val)\n",
    "        self.q[prev_index, self.prev_action] = max_val\n",
    "        \n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        \"\"\"\n",
    "        A method used to pass information from the agent to the experiment.\n",
    "        Args:\n",
    "            message: The message passed to the agent\n",
    "        Returns:\n",
    "            The response (or answer) to the message\n",
    "        \"\"\"\n",
    "        if message == \"get_values\":\n",
    "            return self.q\n",
    "        else:\n",
    "            raise Exception(\"TDAgent.agent_message(): Message not understood!\")\n",
    "\n",
    "    # Helper method\n",
    "    def argmax(self, q_values):        \n",
    "        \"\"\"\n",
    "        argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action-values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)\n",
    "        \n",
    "    # Helper method\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"\n",
    "        Converts the state into an index\n",
    "        Args:\n",
    "            state ([int, int]]): the state to transform\n",
    "        Returns:\n",
    "            index (int): the index of the state\n",
    "        \"\"\"\n",
    "        \n",
    "        return int(state[0] * 10 + state[1])\n",
    "    \n",
    "    def index_to_state(self, index):\n",
    "        \"\"\"\n",
    "        Converts the index into the state\n",
    "        Args:\n",
    "            index (int): the index of the state\n",
    "        Returns:\n",
    "            state ([int, int]]): the state to transform\n",
    "        \"\"\"\n",
    "        \n",
    "        return [int(index/10), (index%10)]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaLearningAgent(BaseAgent):\n",
    "    \n",
    "    def agent_init(self, agent_info={}):\n",
    "        \"\"\"\n",
    "        Setup for the agent called when the experiment first starts.\n",
    "        Args:\n",
    "            agent_info: Contains the information for the agent\n",
    "        \"\"\"\n",
    "    \n",
    "        # Defining the #actions and #states \n",
    "        self.num_actions = 4\n",
    "        self.num_states = 70\n",
    "        \n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\", 1)\n",
    "\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
    "\n",
    "        # To control the exploration-exploitation trade-off\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        \n",
    "        # Defining the initial action values\n",
    "        self.q = np.zeros((self.num_states, self.num_actions))\n",
    "        \n",
    "        # Defining a random generator\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\", 0))\n",
    "        \n",
    "        ### DEBUGGING CODE\n",
    "        # print(self.discount, self.step_size, self.epsilon)\n",
    "    \n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"\n",
    "        The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (int): the state observation from the environment's env_start method.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert the state to an index\n",
    "        index = self.state_to_index(state)\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[index][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "            \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"\n",
    "        A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (int): the state observation from the environment's step \n",
    "                based on where the agent ended up after the last step.\n",
    "        Returns:\n",
    "            action (int): the action the agent is taking.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert the state to an index\n",
    "        index = self.state_to_index(state)\n",
    "        prev_index = self.state_to_index(self.prev_state)\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[index][:]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "\n",
    "        # Perform an update\n",
    "        self.q[prev_index, self.prev_action] += self.step_size * (\n",
    "            reward + self.discount * self.q[index, action] - self.q[prev_index, self.prev_action]\n",
    "        )\n",
    "\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"\n",
    "        Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert the state to an index\n",
    "        prev_index = self.state_to_index(self.prev_state)  \n",
    "        \n",
    "        # Perform an update\n",
    "        self.q[prev_index, self.prev_action] += self.step_size * (\n",
    "            reward + self.discount * 0 - self.q[prev_index, self.prev_action]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        \"\"\"\n",
    "        A method used to pass information from the agent to the experiment.\n",
    "        Args:\n",
    "            message: The message passed to the agent\n",
    "        Returns:\n",
    "            The response (or answer) to the message\n",
    "        \"\"\"\n",
    "        if message == \"get_values\":\n",
    "            return self.q\n",
    "        else:\n",
    "            raise Exception(\"TDAgent.agent_message(): Message not understood!\")\n",
    "\n",
    "    # Helper method\n",
    "    def argmax(self, q_values):        \n",
    "        \"\"\"\n",
    "        argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action-values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)\n",
    "        \n",
    "    # Helper method\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"\n",
    "        Converts the state into an index\n",
    "        Args:\n",
    "            state ([int, int]]): the state to transform\n",
    "        Returns:\n",
    "            index (int): the index of the state\n",
    "        \"\"\"\n",
    "        \n",
    "        return int(state[0] * 10 + state[1])\n",
    "    \n",
    "    def index_to_state(self, index):\n",
    "        \"\"\"\n",
    "        Converts the index into the state\n",
    "        Args:\n",
    "            index (int): the index of the state\n",
    "        Returns:\n",
    "            state ([int, int]]): the state to transform\n",
    "        \"\"\"\n",
    "        \n",
    "        return [int(index/10), (index%10)]       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8886dfe",
   "metadata": {},
   "source": [
    "# 3. Plotting Functions\n",
    "- In this section, I have defined 2 plotting functions:\n",
    "    - `plt_opt_policy`: It plots the optimal policy corresponding to the current Q-Values.\n",
    "    - `plt_opt_val`: It plots the optimal state values corresponding to the current Q-Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba61c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_index(state):\n",
    "    return int(state[0] * 10 + state[1])\n",
    "\n",
    "def index_to_state(index):\n",
    "    return [int(index/10), (index%10)]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e692a6c",
   "metadata": {},
   "source": [
    "# 4. Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env_info, agent_info, num_episode = 1000, plot = True):\n",
    "    env = WindyGridEnvironment\n",
    "    \n",
    "    # Defining the agent\n",
    "    # agent = QLearningAgent\n",
    "    agent = SarsaLearningAgent\n",
    "    \n",
    "    rl_glue = RLGlue(env, agent)\n",
    "\n",
    "    rl_glue.rl_init(agent_info, env_info)\n",
    "    \n",
    "    for episode in range(1, num_episode + 1):\n",
    "        rl_glue.rl_episode(0)\n",
    "        num_steps = rl_glue.rl_num_steps()\n",
    "        print(f\"Episode = {episode} | #Steps Taken = {num_steps}\")\n",
    "\n",
    "        q_vals = rl_glue.agent.agent_message(\"get_values\")\n",
    "        q_random = np.reshape(q_vals, (7, 10, 4))\n",
    "        actions = np.argmax(q_random, axis = -1)\n",
    "        values = np.max(q_random, axis = -1)\n",
    "        print(\"Actions\", actions)\n",
    "        print(\"Values\", values)\n",
    "        print()\n",
    "        \n",
    "    return q_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd8a89",
   "metadata": {},
   "source": [
    "- 0: UP\n",
    "- 1: RIGHT\n",
    "- 2: DOWN\n",
    "- 3: LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3ffa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the characteristics for the environment\n",
    "env_info = {}\n",
    "\n",
    "# Defining the characteristics for the agent\n",
    "agent_info = {\n",
    "    \"discount\": 1,       # UNDISCOUNTED\n",
    "    \"step_size\": 0.5,\n",
    "    \"epsilon\": 0.2,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "q_optimal = run_experiment(env_info, agent_info, num_episode = 5, plot = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
